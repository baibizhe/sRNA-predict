import sys
import math
from torch.utils import data

import anndata
import torch
import argparse
import collections
import numpy as np
import scanpy as sc
import pandas as pd
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit

"""
Written based on the original data processing done by ACTINN 
to preserve compatibility with datasets processed by the TF version

"""


class Mycelldataset(data.Dataset):
    def __init__(self, data_and_labels):
        self.imgs = data_and_labels[:, 0]
        self.labels = data_and_labels[:, 1]
        # self.transforms = transform

    def __getitem__(self, index):
        img = self.imgs[index]
        label = self.labels[index]

        return torch.tensor(img), torch.tensor(label)

    def __len__(self):
        return len(self.imgs)


def type2label_dict(types):
    """
    Turn types into labels
    INPUT:
        types-> types of cell present in the data

    RETURN
     celltype_to_label_dict-> type_to_label dictionary

    """

    all_celltype = list(np.unique(types))
    celltype_to_label_dict = {}

    for i in range(len(all_celltype)):
        celltype_to_label_dict[all_celltype[i]] = i
    return celltype_to_label_dict


def convert_type2label(types, type_to_label_dict):
    """
    Convert types to labels
    INPUTS:
        types-> list of types
        type_to_label dictionary-> dictionary of cell types mapped to numerical labels

    RETURN:
        labels-> list of labels

    """

    types = list(types)
    labels = list()
    for type in types:
        labels.append(type_to_label_dict[type[0]])
    return labels


def scale_sets(total_set,label):
    """
    Get common genes, normalize  and scale the sets
    INPUTS:
        sets-> a list of all the sets to be scaled

    RETURN:
        sets-> normalized sets
    """
    #
    # common_genes = set(sets[0].index)
    # for i in range(1, len(sets)):
    #     common_genes = set.intersection(set(sets[i].index), common_genes)
    # common_genes = sorted(list(common_genes))

    # total_set = np.array(pd.concat(sets, axis=0, sort=False), dtype=np.float32)
    total_set = np.divide(np.array(total_set), np.expand_dims(np.array(np.sum(total_set, axis=1)),axis=1)) * 20000
    total_set = np.log2(total_set + 1)
    expr = np.sum(total_set, axis=1)
    total_set = total_set[np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99)),]
    label = label[np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99))]
    cv = np.std(total_set, axis=1) / np.mean(total_set, axis=1)
    total_set = total_set[np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99)),]
    label = label[np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99))]

    return total_set,label


def CSV_IO(train_path: str, train_labels_path: str, test_path: str, test_labels_path: str,
           batchSize: int = 128, workers: int = 12, fold=1,n_com = 1000):
    """
    This function allows the use of data that was generated by the original ACTINN code (in TF)

    INPUTS
        train_path-> path to the h5 file for the training data (dataframe of Genes X Cells)
        train_labels_path-> path to the csv file of the training data labels (cell type strings)
        test_path-> path to the h5 file of the testing data (dataframe of Genes X Cells)
        test_labels_path-> path to the csv file of the testl dataabels (cell type strings)

    RETURN
        train_data_loader-> training data loader consisting of the data (at batch[0]) and labels (at batch[1])
        test_data_loader-> testing data loader consisting of the data (at batch[0]) and labels (at batch[1])

    """

    slices = 20000
    print("==> Reading in H5ad Data frame (CSV)")
    data = sc.read_h5ad(train_path)[0:slices]
    varnames = data.var_names

    sc.pp.normalize_total(data,inplace=True)
    sc.pp.pca(data=data,n_comps = n_com,zero_center=False)
    data = data.obsm['X_pca']

    # data = data.to_df()

    # with open("drop_indces.txt") as f:
    #     drop_indces = f.readlines()
    # drop_indces = list(map(lambda x: bool(int(x.strip()[0:1])), drop_indces))
    # data = data.iloc[:,drop_indces]
    # print("dropped gene count", len(drop_indces))
    data_label = pd.read_csv(train_labels_path)[0:slices]
    data_label.drop("level2", axis=1, inplace=True)
    data_label.drop("level3", axis=1, inplace=True)
    data_label.drop("level4", axis=1, inplace=True)
    data_label.drop("cell_id", axis=1, inplace=True)

    data = pd.DataFrame(data=data,columns=varnames[0:n_com])

    # data, data_label = scale_sets(data, data_label)
    data_label = data_label.values
    # data = pd.DataFrame(data=data,columns=varnames)

    skf = StratifiedShuffleSplit(n_splits=5,test_size=0.15)
    skf.get_n_splits(data_label, data_label)
    print("==> training on fold", fold)
    for _ in range(fold):
        train_idx, valid_idx = skf.split(data_label, data_label).__next__()


    print("==> validation index", valid_idx)
    #     train_idx, valid_idx = skf.split(data_label.level1, data_label.level1).__next__()

    train_set = data.iloc[train_idx]


    # train_set = pd.read_hdf(train_path, key="dge")
    # train_set.index = [s.upper() for s in train_set.index]
    # train_set = train_set.loc[~train_set.index.duplicated(keep='first')]

    test_set = data.iloc[valid_idx]

    # test_set.index = [s.upper() for s in test_set.index]
    # test_set = test_set.loc[~test_set.index.duplicated(keep='first')]
    data = None
    train_label = data_label[train_idx]

    test_label = data_label[valid_idx]




    type_to_label_dict = type2label_dict(train_label)
    # label_to_type_dict = {v: k for k, v in type_to_label_dict.items()}
    print(f"    -> Cell types in training set: {type_to_label_dict}")
    print(f"    -> # trainng cells: {train_label.shape[0]}")
    names = list(np.unique(train_label))
    count_train = []
    count_valid = []
    for i in names:
        count_train.append((train_label == i).sum())
        count_valid.append((test_label == i).sum())
    for i in range(len(names)):
        print(names[i], " {} in train ".format(count_train[i]), "{} in valid".format(count_valid[i]))
    print("total length", len(train_label))
    train_label = convert_type2label(train_label, type_to_label_dict)
    test_label = convert_type2label(test_label, type_to_label_dict)
    # with open('type_to_label_dict.txt', 'w') as f:
    #     f.writelines(str(type_to_label_dict))
    # f.close()
    # we want to get Cells X Genes
    # train_set = np.transpose(train_set).T
    # test_set = np.transpose(test_set)
    print(f"    *** Remember we the data is formatted as Cells X Genes ***")

    data_and_labels = []
    input_size = train_set.iloc[0].shape[0]
    validation_data_and_labels = []
    for i in range(len(train_set)):
        data_and_labels.append([np.array(train_set.iloc[i]), np.array(train_label[i])])

    for i in range(len(test_set)):
        validation_data_and_labels.append([np.array(test_set.iloc[i]), np.array(test_label[i])])

    # create DataLoaders
    train_dataset = Mycelldataset(np.array(data_and_labels,dtype=object))
    test_dataset = Mycelldataset(np.array(validation_data_and_labels,dtype=object))

    train_data_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, sampler=None,
                                   batch_sampler=None, num_workers=workers, collate_fn=None, prefetch_factor=4,
                                   pin_memory=True)

    test_data_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False, sampler=None,
                                  batch_sampler=None, num_workers=workers, collate_fn=None, prefetch_factor=4,
                                  pin_memory=True)

    return train_data_loader, test_data_loader, input_size, type_to_label_dict
