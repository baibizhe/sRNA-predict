import sys
import math
from torch.utils import data

import anndata
import torch
import argparse
import collections
import numpy as np
import scanpy as sc
import pandas as pd
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit
from scipy.sparse import csr_matrix,bmat

"""
Written based on the original data processing done by ACTINN 
to preserve compatibility with datasets processed by the TF version

"""


class Mycelldataset(data.Dataset):
    def __init__(self, data,labels):
        self.data = data
        self.labels = labels
        # self.transforms = transform

    def __getitem__(self, index):
        img = self.data[index]
        label = self.labels[index]

        return torch.tensor(img.toarray()), torch.tensor(label)

    def __len__(self):
        return self.data.shape[0]


def type2label_dict(types):
    """
    Turn types into labels
    INPUT:
        types-> types of cell present in the data

    RETURN
     celltype_to_label_dict-> type_to_label dictionary

    """

    all_celltype = list(np.unique(types))
    celltype_to_label_dict = {}

    for i in range(len(all_celltype)):
        celltype_to_label_dict[all_celltype[i]] = i
    return celltype_to_label_dict


def convert_type2label(types, type_to_label_dict):
    """
    Convert types to labels
    INPUTS:
        types-> list of types
        type_to_label dictionary-> dictionary of cell types mapped to numerical labels

    RETURN:
        labels-> list of labels

    """

    types = list(types)
    labels = list()
    for type in types:
        labels.append(type_to_label_dict[type[0]])
    return labels

def vars(a, axis=None):
    """ Variance of sparse matrix a
    var = mean(a**2) - mean(a)**2
    """
    a_squared = a.copy()
    a_squared.data **= 2
    return a_squared.mean(axis) - np.square(a.mean(axis))

def stds(a, axis=None):
    """ Standard deviation of sparse matrix a
    std = sqrt(var(a))
    """
    return np.sqrt(vars(a, axis))
def scale_sets(total_set,label):
    """
    Get common genes, normalize  and scale the sets
    INPUTS:
        sets-> a list of all the sets to be scaled

    RETURN:
        sets-> normalized sets
    """
    #
    # common_genes = set(sets[0].index)
    # for i in range(1, len(sets)):
    #     common_genes = set.intersection(set(sets[i].index), common_genes)
    # common_genes = sorted(list(common_genes))

    # total_set = np.array(pd.concat(sets, axis=0, sort=False), dtype=np.float32)
    total_set / total_set.sum(1) * 20000
    # total_set = np.divide(np.array(total_set), np.expand_dims(np.array(np.sum(total_set, axis=1)),axis=1)) * 20000
    total_set = total_set.log1p()
    expr = np.array(total_set.sum(1)).flatten()
    total_set = total_set[np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99)),]
    label = label[np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99))]
    cv = np.array(np.array(stds(total_set,axis=1))/total_set.mean(1)).flatten()
    total_set = total_set[np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99)),]
    label = label[np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99))]

    return total_set,label


def CSV_IO(train_path: str, train_labels_path: str, test_path: str, test_labels_path: str,
           batchSize: int = 128, workers: int = 12, fold=1):
    """
    This function allows the use of data that was generated by the original ACTINN code (in TF)

    INPUTS
        train_path-> path to the h5 file for the training data (dataframe of Genes X Cells)
        train_labels_path-> path to the csv file of the training data labels (cell type strings)
        test_path-> path to the h5 file of the testing data (dataframe of Genes X Cells)
        test_labels_path-> path to the csv file of the testl dataabels (cell type strings)

    RETURN
        train_data_loader-> training data loader consisting of the data (at batch[0]) and labels (at batch[1])
        test_data_loader-> testing data loader consisting of the data (at batch[0]) and labels (at batch[1])

    """

    slices = 40000
    print("==> Reading in H5ad Data frame (CSV)")
    data = sc.read_h5ad(train_path)[0:slices].X
    varnames = sc.read_h5ad(train_path).var_names

    test_data = sc.read_h5ad(test_path)[0:slices].X
    alldata = bmat([[data], [test_data]], format="csr")
    zero_indexs = np.array(np.sum(alldata, axis=0) > 0).flatten()
    alldata=None
    data = data[:,zero_indexs]
    test_data = test_data[:,zero_indexs]


    genes_zero = []
#     for gene in data:
#         if np.sum(data[gene]) == 0:
#             genes_zero.append(gene)
#             count += 1
#     data = data.drop(genes_zero, axis=1)
    print("dropped gene count", len(zero_indexs)-zero_indexs.sum())
    data_label = pd.read_csv(train_labels_path)[0:slices]
    data_label.drop("level2", axis=1, inplace=True)
    data_label.drop("level3", axis=1, inplace=True)
    data_label.drop("level4", axis=1, inplace=True)
    data_label.drop("cell_id", axis=1, inplace=True)
    data, data_label = scale_sets(data, data_label)
    data_label = data_label.values #
    # data = pd.DataFrame(data=data,columns=varnames)

    skf = StratifiedShuffleSplit(n_splits=5,test_size=0.15)
    skf.get_n_splits(data_label, data_label)
    print("==> training on fold", fold)
    for _ in range(fold):
        train_idx, valid_idx = skf.split(data_label, data_label).__next__()


    # print("==> validation index", valid_idx)
    #     train_idx, valid_idx = skf.split(data_label.level1, data_label.level1).__next__()

    train_set = data[train_idx]


    # train_set = pd.read_hdf(train_path, key="dge")
    # train_set.index = [s.upper() for s in train_set.index]
    # train_set = train_set.loc[~train_set.index.duplicated(keep='first')]

    test_set = data[valid_idx]

    # test_set.index = [s.upper() for s in test_set.index]
    # test_set = test_set.loc[~test_set.index.duplicated(keep='first')]
    data = None
    train_label = data_label[train_idx]

    test_label = data_label[valid_idx]




    type_to_label_dict = type2label_dict(train_label)
    # label_to_type_dict = {v: k for k, v in type_to_label_dict.items()}
    print(f"    -> Cell types in training set: {type_to_label_dict}")
    print(f"    -> # trainng cells: {train_label.shape[0]}")
    names = list(np.unique(train_label))
    count_train = []
    count_valid = []
    for i in names:
        count_train.append((train_label == i).sum())
        count_valid.append((test_label == i).sum())
    for i in range(len(names)):
        print(names[i], " {} in train ".format(count_train[i]), "{} in valid".format(count_valid[i]))
    train_label = convert_type2label(train_label, type_to_label_dict)
    test_label = convert_type2label(test_label, type_to_label_dict)
    with open('type_to_label_dict.txt', 'w') as f:
        f.writelines(str(type_to_label_dict))
    f.close()
    # we want to get Cells X Genes
    # train_set = np.transpose(train_set).T
    # test_set = np.transpose(test_set)
    print(f"    *** Remember we the data is formatted as Cells X Genes ***")

    data_and_labels = []
    input_size = train_set[0].shape[1]
    validation_data_and_labels = []
    # for i in range(len(train_set)):
    #     data_and_labels.append([np.array(train_set.iloc[i]), np.array(train_label[i])])
    #
    # for i in range(len(test_set)):
    #     validation_data_and_labels.append([np.array(test_set.iloc[i]), np.array(test_label[i])])

    # create DataLoaders
    train_dataset = Mycelldataset(train_set,train_label)
    test_dataset = Mycelldataset(test_set,test_label)
    print("total  train length", len(train_dataset) , len(test_dataset))

    train_data_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, sampler=None,
                                   batch_sampler=None, num_workers=workers, collate_fn=None, prefetch_factor=4,
                                   pin_memory=True)

    test_data_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False, sampler=None,
                                  batch_sampler=None, num_workers=workers, collate_fn=None, prefetch_factor=4,
                                  pin_memory=True)

    return train_data_loader, test_data_loader, input_size, type_to_label_dict
